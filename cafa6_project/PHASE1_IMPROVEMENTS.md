# Phase 1 Improvements - GPU Optimization & Benchmarking

## Overview

This document describes the improvements made to Phase 1 of the CAFA-6 Protein Function Prediction project. The enhancements focus on GPU optimization, comprehensive logging, CPU vs GPU benchmarking, and profiling integration for the GPU programming project.

---

## Changes Summary

### **Files Modified (3)**

1. **`generate_embeddings.py`** (168 â†’ 473 lines)
   - Added CPU/GPU mode selection (`--device cpu/cuda`)
   - Comprehensive timing and throughput metrics
   - Memory tracking and reporting
   - JSON performance report export
   - Warmup iterations (excluded from metrics)
   - Advanced GPU optimizations (cuDNN benchmark, TF32)
   - YAML configuration file support
   - Structured logging to file
   - Better error handling

2. **`analyze_gpu_logs.py`** (78 â†’ 143 lines)
   - **FIXED:** Column name bugs (`gpu_mem_gb` instead of `gpu_mem_used_gb`)
   - **FIXED:** Hardcoded filename (now supports `--log` argument)
   - Added thermal throttling detection
   - Better error messages and validation
   - Flexible output paths

3. **`concat_embeddings.py`** (59 â†’ 90 lines)
   - ID validation across models (detects mismatches)
   - NaN/Inf detection
   - Timing and throughput reporting
   - Better error handling

### **New Files Created (3)**

4. **`benchmark_cpu_gpu.py`** (NEW, 330 lines)
   - Side-by-side CPU vs GPU comparison
   - Quantitative speedup measurement
   - Visualization of throughput and speedup
   - JSON results export
   - **Critical for GPU programming project report**

5. **`profile_embeddings.py`** (NEW, 290 lines)
   - `torch.profiler` integration
   - Chrome trace export for visualization
   - Kernel-level performance analysis
   - Memory pattern identification
   - Bottleneck detection for custom CUDA kernels

6. **`config.yaml`** (NEW, 60 lines)
   - Centralized configuration
   - Model settings and batch sizes
   - Optimization flags
   - Easy tuning without code changes

### **Dependencies Added**

Updated `requirements.txt`:
- `pyyaml` - Configuration file parsing
- `matplotlib` - Visualization
- `psutil` - System monitoring (already used in gpu_monitor.py)

---

## New Features

### 1. CPU vs GPU Mode

```bash
# Run on GPU (default)
python generate_embeddings.py --device cuda

# Run on CPU (for baseline comparison)
python generate_embeddings.py --device cpu
```

### 2. Comprehensive Metrics

**Output now includes:**
- Total time, proteins/sec, batches/sec, tokens/sec
- Per-batch timing statistics (mean, std, min, max)
- GPU memory peak and average
- Memory utilization percentage
- Warmup iterations (excluded from metrics)

**Example output:**
```
PERFORMANCE SUMMARY: esm2_650m
======================================================================
Total proteins: 82,404
Total batches: 5,150 (warmup: 3)
Total time: 4981.23s (83.0m)

Throughput:
  Proteins/sec: 984.2
  Batches/sec: 61.5
  Tokens/sec: 347,582.1

Batch timing:
  Average: 0.982s
  Std dev: 0.124s
  Min: 0.845s
  Max: 1.456s

Memory usage:
  Peak: 8.23GB
  Average: 7.89GB
  Utilization: 68.6%
======================================================================
```

### 3. JSON Performance Reports

All metrics are saved to `outputs/embedding_performance_report_{device}.json`:

```json
{
  "esm2_650m": {
    "train": {
      "total_time": 4981.23,
      "proteins_per_sec": 984.2,
      "peak_memory_gb": 8.23,
      ...
    },
    "test": { ... }
  },
  ...
}
```

### 4. Structured Logging

Logs are saved to `outputs/logs/embedding_generation_{device}_{timestamp}.log`:

```
2025-10-23 14:23:45 - __main__ - INFO - Device: cuda
2025-10-23 14:23:45 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-10-23 14:23:45 - __main__ - INFO - cuDNN benchmark enabled
2025-10-23 14:23:45 - __main__ - INFO - TensorFloat32 matmul enabled
...
```

### 5. Advanced GPU Optimizations

Automatically enabled:
- `torch.backends.cudnn.benchmark = True` - Auto-tune kernels
- `torch.set_float32_matmul_precision('high')` - TensorFloat32 for RTX GPUs
- Half-precision (fp16) inference
- `torch.compile()` for kernel fusion (non-Windows)
- Mixed precision autocast

### 6. Configuration File

Edit `config.yaml` to tune settings without code changes:

```yaml
models:
  esm2_650m:
    batch_size: 16  # Adjust for your GPU

optimization:
  warmup_batches: 3
  use_amp: true
  cudnn_benchmark: true
```

---

## New Scripts Usage

### **benchmark_cpu_gpu.py** - CPU vs GPU Comparison ðŸ”¥

**Purpose:** Prove GPU acceleration for your project report

```bash
# Quick benchmark (1000 proteins, esm2_650m only)
python benchmark_cpu_gpu.py

# Full benchmark (all models)
python benchmark_cpu_gpu.py --subset 2000 --models esm2_650m esm2_3b protbert ankh

# Custom settings
python benchmark_cpu_gpu.py --subset 500 --batch-size 4 --models esm2_3b
```

**Output:**
- `outputs/cpu_gpu_benchmark_results.json` - Quantitative data
- `outputs/cpu_gpu_comparison.png` - Visualization
- Console output with speedup calculations

**Example output:**
```
SPEEDUP SUMMARY: esm2_650m
============================================================
CPU time: 245.67s
GPU time: 7.89s
Speedup: 31.13x faster
Time saved: 237.78s
============================================================
```

### **profile_embeddings.py** - Kernel Profiling

**Purpose:** Identify bottlenecks for custom CUDA kernel optimization

```bash
# Profile a model (10 batches)
python profile_embeddings.py --model esm2_650m

# Detailed profiling
python profile_embeddings.py --model esm2_650m --num-batches 20 --batch-size 8
```

**Output:**
- `outputs/profiler_trace_{model}.json` - Chrome trace
- Console summary of top operations
- Optimization suggestions

**View trace:**
1. Open `chrome://tracing` in Chrome/Edge
2. Load the JSON file
3. Analyze kernel execution timeline

**Example insights:**
```
KEY INSIGHTS FOR OPTIMIZATION
======================================================================

Tokenization: 45.23ms
  â†’ Potential optimization: Custom tokenizer or caching

Pooling: 12.34ms
  â†’ Potential optimization: Custom CUDA reduction kernel

CPU Transfer: 8.91ms
  â†’ Potential optimization: Pinned memory, async transfers

Matrix Multiplications: 892.45ms
  â†’ Using Tensor Cores (fp16/bf16)
  â†’ Already optimized by cuBLAS
```

### **analyze_gpu_logs.py** - Fixed & Enhanced

```bash
# Analyze GPU monitoring logs
python analyze_gpu_logs.py --log ../outputs/gpu_monitor_phase1.csv

# Custom output path
python analyze_gpu_logs.py --log ../outputs/gpu_monitor.csv --output ../outputs/my_analysis.png
```

**Improvements:**
- âœ… Fixed column name bugs
- âœ… Thermal throttling detection
- âœ… Memory efficiency calculation
- âœ… Better error messages

---

## Workflow Examples

### **Quick Start - Generate Embeddings with Logging**

```bash
cd cafa6_project/scripts

# Generate embeddings with all new features
python generate_embeddings.py --device cuda

# Check performance report
cat ../outputs/embedding_performance_report_cuda.json
```

### **Full CPU vs GPU Comparison**

```bash
# 1. Benchmark one model
python benchmark_cpu_gpu.py --models esm2_650m --subset 1000

# Output:
# - Speedup calculation
# - JSON results
# - Comparison plots
```

### **Profile for Custom CUDA Kernels**

```bash
# 1. Profile the model
python profile_embeddings.py --model esm2_650m --num-batches 10

# 2. Open chrome://tracing and load the JSON file

# 3. Identify bottlenecks:
#    - Attention operations
#    - Pooling operations
#    - Memory transfers

# 4. Implement custom CUDA kernels for bottlenecks (Phase 2)
```

### **Monitor GPU During Training**

```bash
# Terminal 1: Start GPU monitor
python gpu_monitor.py

# Terminal 2: Run embedding generation
python generate_embeddings.py --device cuda

# Terminal 1: Stop monitor (Ctrl+C)

# Analyze logs
python analyze_gpu_logs.py
```

---

## Configuration Options

### **generate_embeddings.py Arguments**

```bash
python generate_embeddings.py --help

--device {cuda,cpu}       Device to use (default: cuda)
--config CONFIG           Path to config.yaml (default: ../config.yaml)
--models MODEL [MODEL...] Specific models to run (default: all)
```

### **benchmark_cpu_gpu.py Arguments**

```bash
python benchmark_cpu_gpu.py --help

--fasta FASTA            Path to FASTA file
--subset SUBSET          Number of sequences (default: 1000)
--models MODEL [MODEL...] Models to benchmark
--batch-size SIZE        Batch size (default: 8)
--output OUTPUT          Output JSON path
```

### **profile_embeddings.py Arguments**

```bash
python profile_embeddings.py --help

--fasta FASTA            Path to FASTA file
--model MODEL            Model to profile
--batch-size SIZE        Batch size (default: 8)
--num-batches NUM        Batches to profile (default: 10)
--output OUTPUT          Output directory
```

---

## Log Files Reference

All log files are persistent and can be accessed later:

| File | Description | Location | Format |
|------|-------------|----------|--------|
| Performance report | Detailed metrics | `outputs/embedding_performance_report_{device}.json` | JSON |
| Execution log | Console output | `outputs/logs/embedding_generation_{device}_{timestamp}.log` | Text |
| GPU monitoring | Real-time metrics | `outputs/gpu_monitor_phase1.csv` | CSV |
| GPU analysis | Visualizations | `outputs/gpu_analysis.png` | PNG |
| Benchmark results | CPU vs GPU data | `outputs/cpu_gpu_benchmark_results.json` | JSON |
| Benchmark plots | Speedup charts | `outputs/cpu_gpu_comparison.png` | PNG |
| Profiler trace | Kernel timeline | `outputs/profiler_trace_{model}.json` | JSON (Chrome) |

**All logs use timestamps or unique names - never overwritten!**

---

## GPU Programming Project Deliverables

### **What You Now Have:**

âœ… **CPU baseline benchmarks** - `benchmark_cpu_gpu.py`
âœ… **Quantitative speedup data** - JSON results with exact measurements
âœ… **Visualization** - Bar charts for throughput and speedup
âœ… **Profiling** - Kernel-level analysis with `torch.profiler`
âœ… **Comprehensive metrics** - Memory, timing, throughput
âœ… **Production logging** - Structured logs for analysis

### **For Your Report:**

1. **Quantitative Results:**
   - Run `benchmark_cpu_gpu.py` to get CPU vs GPU speedup
   - Use JSON data for tables in your report
   - Include comparison plots

2. **Performance Analysis:**
   - Run `profile_embeddings.py` for kernel breakdown
   - Identify bottlenecks for Phase 2 custom CUDA kernels
   - Show before/after optimization comparisons

3. **Optimization Documentation:**
   - Document all GPU optimizations (cuDNN, TF32, fp16, torch.compile)
   - Show memory efficiency improvements
   - Demonstrate thermal management

---

## Troubleshooting

### **Issue: Column name errors in analyze_gpu_logs.py**
**Fixed!** Now uses correct column names from `gpu_monitor.py`

### **Issue: Config file not found**
```bash
# Create from template
cp config.yaml.example config.yaml

# Or let generate_embeddings.py use defaults
python generate_embeddings.py  # Will use default config
```

### **Issue: CUDA out of memory**
```bash
# Reduce batch sizes in config.yaml
models:
  esm2_650m:
    batch_size: 8  # Reduce from 16
  esm2_3b:
    batch_size: 2  # Reduce from 4
```

### **Issue: Slow CPU benchmarking**
```bash
# Use smaller subset
python benchmark_cpu_gpu.py --subset 100
```

---

## Next Steps

### **Phase 1 - Immediate:**
1. Run benchmark to get baseline speedup numbers
2. Profile models to identify bottlenecks
3. Document current performance

### **Phase 2 - Custom CUDA Kernels:**
Based on profiling results, implement:
1. **Smith-Waterman sequence alignment** (replace BLAST)
2. **Custom pooling kernel** (replace PyTorch mean)
3. **GO graph propagation kernel** (for hierarchical constraints)

### **Phase 3 - Integration:**
1. Integrate custom kernels into pipeline
2. Benchmark custom kernels vs library
3. Document performance improvements

---

## Performance Expectations

Based on our hardware (RTX 5070 Ti, 12GB VRAM):

| Model | CPU (proteins/sec) | GPU (proteins/sec) | Expected Speedup |
|-------|-------------------|-------------------|------------------|
| ESM-2 650M | ~30 | ~1000 | ~30x |
| ESM-2 3B | ~10 | ~400 | ~40x |
| ProtBERT | ~40 | ~800 | ~20x |
| Ankh Large | ~35 | ~750 | ~20x |

**Note:** Actual numbers may vary. Run `benchmark_cpu_gpu.py` for exact measurements on your system.

---

## Questions?

**For issues:**
- Check log files in `outputs/logs/`
- Review error messages in console
- Verify CUDA availability: `python test_setup.py`

**For optimization:**
- Run profiler to identify bottlenecks
- Check GPU utilization in monitoring logs
- Tune batch sizes in `config.yaml`

**For GPU programming project:**
- Use `benchmark_cpu_gpu.py` for quantitative speedup data
- Use `profile_embeddings.py` for kernel analysis
- Document all optimizations in your report
